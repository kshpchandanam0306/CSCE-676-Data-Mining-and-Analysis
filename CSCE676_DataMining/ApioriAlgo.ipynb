{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## CSCE 676 :: Data Mining and Analysis :: Texas A&M University :: Spring 2022\n",
        "\n",
        "\n",
        "# Homework 1: Let's GOOOOO!\n",
        "\n",
        "- **100 points [7% of your final grade]**\n",
        "- **Due Tuesday, February 13 by 11:59pm**\n",
        "\n",
        "***Goals of this homework:***\n",
        "1. Collect data from the web, clean it, and then make some observations based on exploratory data analysis\n",
        "2. Understand and implement the classic apriori algorithm and extensions to find the association rules in a movie rating dataset\n",
        "\n",
        "***Submission instructions:***\n",
        "\n",
        "You should post your notebook to Canvas (look for the homework 1 assignment there). Please name your submission **your-uin_hw1.ipynb**, so for example, my submission would be something like **555001234_hw1.ipynb**. Your notebook should be fully executed when you submit ... so run all the cells for us so we can see the output, then submit that. \n",
        "\n",
        "***Late Days:***\n",
        "\n",
        "As a reminder, you may use up to three of your late days on this homework, meaning the latest we will accept it is February 16 by 11:59pm.\n",
        "\n",
        "***Collaboration declaration:***\n",
        "\n",
        "If you worked with someone on this homework, please be sure to mention that. Remember to include citations to any sources you use in the homework."
      ],
      "metadata": {
        "id": "Ak3SqhmRDYKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (50 points) Part 1: UFOs"
      ],
      "metadata": {
        "id": "iiUzZn-6Ecs8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (10pts) Part 1a: UFOs are Out There, But First I Need to Store them Locally\n",
        "\n",
        "For this first part, we're going to collect evidence of UFO sightings from the **National UFO Reporting Center**. Specifically, we're going \n",
        "to focus only on UFO sightings in Texas, as reported at this webpage:\n",
        "\n",
        "* http://www.nuforc.org/webreports/ndxlTX.html\n",
        "\n",
        "Recall that you can view the source of a webpage in Chrome under View &rarr; Developer &rarr; View Source. \n",
        "You'll notice, however, that this raw HTML is not in our friendly csv format and so will require some initial pre-processing. \n",
        "In particular, we're going to use the Python libraries **[requests](http://docs.python-requests.org/en/master/)** \n",
        "and **[beautiful soup](https://www.crummy.com/software/BeautifulSoup/)** to convert this UFO data from its original HTML format into csv. \n",
        "\n",
        "Hints:\n",
        "* You'll notice that the column headers are in the `<TH>` tags.\n",
        "* The values are in the `<TD>` tags.\n",
        "* In beautiful soup, something like `.find_all('td')` may help you.\n",
        "* To write the csv, you might want to `import csv` and take a look at the functions provided.\n",
        "* If you google for \"beautifulsoup table to csv\" you should find some nice starting points.  Note, however, that you may not use an existing method that auto-magically converts the HTML into csv; we expect you to write your own code. If you borrow some elements from online resources, you should cite them in the comments. "
      ],
      "metadata": {
        "id": "oHXUGi-gm88u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "# you should use requests to get the webpage, then extract \n",
        "# the appropriate column headings and rows\n",
        "# then write this out to csv to a local file called 'ufos_in_texas.csv'"
      ],
      "metadata": {
        "id": "R7e6aYoQEYTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have your local csv file, you should read it in and then issue the .head() command."
      ],
      "metadata": {
        "id": "BfQTWXRfnEZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "lGjL27fsnEIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (15pts) Part 1b: UFOs are a Mess! Time to Clean Up!\n",
        "\n",
        "Okay, now we move to the fun part -- making sense of this messy data. These UFO reports are user-generated with little input validation. As a result, you will notice lots of oddities. \n",
        "\n",
        "Let's begin by focusing on the **Duration** column. As a first pass, let's make a grossly simplifying assumption -- that the only valid data is any duration that is of the form:  \n",
        "\n",
        "* 1 second\n",
        "* 2 seconds\n",
        "* ...\n",
        "* 1 minute\n",
        "* 2 minutes\n",
        "* ...\n",
        "* 1 hour\n",
        "* 2 hours \n",
        "* ...\n",
        "* 1 day\n",
        "* 2 days \n",
        "* ...\n",
        "\n",
        "That is, we will only accept positive integers followed by a space, followed by a properly spelled unit. Every other entry is invalid. For example, that means these are all invalid durations:\n",
        "\n",
        "* 1s\n",
        "* 2 min.\n",
        "* 2-3 seconds\n",
        "* 10-15min\n",
        "* 1 minute+\n",
        "* 30 minutes and longer\n",
        "* about 1.5 minutes\n",
        "\n",
        "You may find the **pandas** library to be very helpful for this part. Create a new pandas dataframe that only includes sightings with these values, **where you convert all durations into seconds**. How many total rows are there in the original dataset? How many rows in your new 'validated' dataset? Report the basic statistics of the duration in your new 'validated' dataset (report maximum, minimum, mean, and standard deviation values of duration). At last, plot a boxplot of the duration (in seconds) in your 'validated' dataset."
      ],
      "metadata": {
        "id": "kCSVPkNOnYfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here \n",
        "# filter our invalid durations\n",
        "# convert all valid durations to seconds\n"
      ],
      "metadata": {
        "id": "wMC-mq6wEnnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "# total rows in original dataset\n",
        "# valid rows in your new 'validated' dataset\n"
      ],
      "metadata": {
        "id": "6Yio0HTHncIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "# report the basic statistics of duration in 'validated' dataset\n",
        "# boxplot code here\n"
      ],
      "metadata": {
        "id": "5VSAJpk6nd6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (15pts) Part 1c: Can we do better?\n",
        "Interesting. But we threw away a **lot** of data. We can do better. For this part, you will do your best to clean up the durations from your original dataset. Keep in mind some initial guidelines:\n",
        "\n",
        "* If a duration has a range, use the average as its value. For example, if the duration is listed as “6-8 minutes”, you should consider the duration as “7 minutes”. (Again, you will need to eventually convert minutes into seconds).\n",
        "* If a duration has a “<” sign, you should simply ignore the “<” sign. For example if the duration is specified as “< 1 minute”, consider the duration to be “1 minute”. You should subsequently convert “1 minute” to \"60 seconds\".\n",
        "* If a duration has a “>” sign, you should simply ignore the “>” sign. \n",
        "* You should ignore any row with an empty duration.\n",
        "\n",
        "You will probably have to improvise as you go along, so **make detailed notes of what decisions you are making and why**."
      ],
      "metadata": {
        "id": "XRED23qlni_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here \n",
        "# clean data \n",
        "# convert cleaned durations to seconds\n"
      ],
      "metadata": {
        "id": "q3zORgUTnmZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "# total rows in original dataset\n",
        "# valid rows in your cleaned dataset\n"
      ],
      "metadata": {
        "id": "e2PDFUk1nqY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "# report the basic statistics of duration in your cleaned dataset\n",
        "# draw a boxplot for your cleaned dataset\n"
      ],
      "metadata": {
        "id": "L_r7ogIHnquU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (5pts) Part 1d: Observations and Conclusions\n",
        "\n",
        "Based on your analysis on part 1b and 1c, what observations or conclusions can you make from the data?"
      ],
      "metadata": {
        "id": "-5rKvm1k7HHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*your answer here*"
      ],
      "metadata": {
        "id": "vrnseH_t7RzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (5pts) Part 1e: Next Steps\n",
        "\n",
        "Now is your chance to conduct an interesting analysis on the UFO data you have collected. This is open-ended, so you may choose whatever direction you like. For example, you might want to take a look at the shape of the UFOs or perhaps the temporal aspects of the reports. "
      ],
      "metadata": {
        "id": "X-n9Zd2j7uFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "GngkwXtD8Kvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*tell us what next steps you took, and what you discovered*"
      ],
      "metadata": {
        "id": "tb6tGIBG8M2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (50 points) Part 2: Association Rules in Movie Rating Behaviors"
      ],
      "metadata": {
        "id": "FQ6FjJ2NEoFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the second part of this homework, we're going to examine movies using our understanding of association rules, to find movies that \"go together\". For this part, you will implement the apriori algorithm, and apply it to a movie rating dataset. We'll use the [MovieLens](https://grouplens.org/datasets/movielens/) dataset.\n",
        "\n",
        "First, run the next cell to load the dataset we are going to use."
      ],
      "metadata": {
        "id": "CKVTACJGFGYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib3\n",
        "import zipfile\n",
        "\n",
        "http = urllib3.PoolManager()\n",
        "req = http.request(\"GET\", \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\", preload_content=False)\n",
        "\n",
        "with open(\"movie.zip\", 'wb') as out:\n",
        "  while True:\n",
        "    data = req.read(4096)\n",
        "    if not data:\n",
        "      break\n",
        "    out.write(data)\n",
        "req.release_conn()\n",
        "\n",
        "zFile = zipfile.ZipFile(\"movie.zip\", \"r\")\n",
        "for fileM in zFile.namelist():\n",
        "  zFile.extract(fileM)"
      ],
      "metadata": {
        "id": "fcZSAoCAFDyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ml-latest-small/"
      ],
      "metadata": {
        "id": "YgIIGQ3lIgRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this dataset, there are four columns: `userId` is the integer ids of users, `movieId` is the integer ids of movies, `rating` is the rate of the user gives to the movie, and `timestamp` which we do not use here. Each row denotes that the user of given `userId` rated the movie of the given `movieId`. We are going to treat each user as a \"basket\", so you will need to collect all the movies that have been rated by a single user as a basket. \n",
        "\n",
        "Now, you need to implement the apriori algorithm and apply it to this dataset to find association rules of user rating behaviors where:\n",
        "\n",
        "1. Define `rating` >= 3 is \"like\" (that is, only consider movie ratings of 3 or higher in your baskets; you may ignore all others)\n",
        "2. `minsup` == 40 (out of 600 users/baskets); we may adjust this based on the discussion on Campuswire\n",
        "3. `minconf` == to be determined by a discussion on Campuswire. You may try several different choices, but we will converge on a good choice for everyone for the final submission.\n",
        " \n",
        "We know there are many existing implementations of apriori online (check github for some good starting points). You are welcome to read existing codebases and let that inform your approach. Do not copy-paste any existing code. We want your code to have sufficient comments to explain your steps, to show us that you really know what you are doing. Furthermore, you should add print statements to print out the intermediate steps of your method -- e.g., the size of the candidate set at each step of the method, the size of the filtered set, and any other important information you think will highlight the method. \n",
        "\n",
        "To help get you started, we can load the ratings with the following code snippet:"
      ],
      "metadata": {
        "id": "RiF1Gc0q7qzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# read user ratings\n",
        "allRatings = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
        "allRatings"
      ],
      "metadata": {
        "id": "0y8yZnEVI3Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (15pts) Step 1: Implement Apriori Algorithm\n",
        "In this section, you need to implement the Apriori algorithm, we will check the correctness of your code and we encourage efficient implementation and skills of pruning."
      ],
      "metadata": {
        "id": "QfwWA4d2Ne6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "48ITOQG09fT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (5pts) Step 2: Print Your Association Rules\n",
        "\n",
        "Next you should print your final association rules in the following format:\n",
        "\n",
        "**movie_name_1, movie_name_2, ... --> \n",
        "movie_name_k**\n",
        "\n",
        "where the movie names can be fetched by joining the movieId with the file `movies.csv`. For example, one rule that you might find is:\n",
        "\n",
        "**Matrix, The (1999),  Star Wars: Episode V - The Empire Strikes Back (1980),  Star Wars: Episode IV - A New Hope (1977),  -> \n",
        "Star Wars: Episode VI - Return of the Jedi (1983)**"
      ],
      "metadata": {
        "id": "Ea6GbdOOBZOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "rSbhiIkw9kj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (15pts) Step 3: Implement Random Sampling\n",
        "\n",
        "We discussed in class a method to randomly sample baskets to avoid the overhead of reading the entire set of baskets (which in practice, could amount to billions of baskets). For this part, you should implement such a random sampling approach that takes a special parameter **alpha** that controls the size of the sample: e.g., alpha = 0.10 means to sample 10% of the baskets (our users, in this case). \n",
        "\n",
        "Vary **alpha** and report the number of frequent itemsets you find and how this compares to the number of frequent itemsets in the entire dataset. What do you discover?\n"
      ],
      "metadata": {
        "id": "FfeufQAxNB82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "bslz87rc9kET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*your discussion here*"
      ],
      "metadata": {
        "id": "rIBMkM-W-8Tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (10pts) Step 4: Check for False Positives\n",
        "\n",
        "Next you should verify that the candidate pairs you discover by random sampling are truly frequent by comparing to the itemsets you discover over the entire dataset. \n",
        "\n",
        "For this part, consider another parameter **minsup_sample** that relaxes the minimum support threshold. For example if we want minsup = 1/100 for whole dataset, then try minsup_sample = 1/125 for the sample. This will help catch truly frequent itemsets.\n",
        "\n",
        "Vary **minsup_sample** and report the number of frequent itemsets you find and the number of false positives you find. What do you discover?\n"
      ],
      "metadata": {
        "id": "wLNDiFDrI2-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "R8gtIPuf_82B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*your discussion here*"
      ],
      "metadata": {
        "id": "tgpWjmXh_91h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (5pts) Step 5: Extensions and Next Steps\n",
        "\n",
        "So far, we have been working with a fairly small dataset. For this last question, try your sampling-based approach on the much larger: **Movies 10M** dataset: https://files.grouplens.org/datasets/movielens/ml-10m.zip\n",
        "\n",
        "First, we need to load this larger dataset:"
      ],
      "metadata": {
        "id": "Wt4JCrwcAHal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib3\n",
        "import zipfile\n",
        "\n",
        "http = urllib3.PoolManager()\n",
        "req = http.request(\"GET\", \"https://files.grouplens.org/datasets/movielens/ml-10m.zip\", preload_content=False)\n",
        "\n",
        "with open(\"movie.zip\", 'wb') as out:\n",
        "  while True:\n",
        "    data = req.read(4096)\n",
        "    if not data:\n",
        "      break\n",
        "    out.write(data)\n",
        "req.release_conn()\n",
        "\n",
        "zFile = zipfile.ZipFile(\"movie.zip\", \"r\")\n",
        "for fileM in zFile.namelist():\n",
        "  zFile.extract(fileM)"
      ],
      "metadata": {
        "id": "iG7qBkj7AVou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls ml-10M100K/"
      ],
      "metadata": {
        "id": "6Hi45CqJht7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# read user ratings\n",
        "allRatings = pd.read_csv(\"ml-10M100K/ratings.dat\",sep='::', names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"], engine='python')\n",
        "allRatings"
      ],
      "metadata": {
        "id": "V_jdR72WiR2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can begin your sampling over this larger dataset."
      ],
      "metadata": {
        "id": "WEX9wu7ewIqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "VYRlIEyulq2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*your discussion here*"
      ],
      "metadata": {
        "id": "FX3LDkfAlpyg"
      }
    }
  ]
}